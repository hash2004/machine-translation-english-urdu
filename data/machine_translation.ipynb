{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UV2_m4fsU1ew",
        "outputId": "f7c49fb1-2be3-4e20-9242-6e5bce27ae23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading and comparing lines from 'english.txt' and 'urdu.txt'...\n",
            "Both files finished. Processed 24524 lines.\n",
            "--------------------\n",
            "Finished reading. Raw lines processed/compared: 24524\n",
            "Filtered English sentences collected: 24524\n",
            "Filtered Urdu sentences collected: 24524\n",
            "--------------------\n",
            "Successfully loaded 24524 aligned sentence pairs.\n",
            "\n",
            "--- Sample Data (First 5 pairs) ---\n",
            "                english                       urdu\n",
            "0   is zain your nephew      زین تمہارا بھتیجا ہے۔\n",
            "1  i wish youd trust me  کاش تم مجھ پر بھروسہ کرتے\n",
            "2      did he touch you      کیا اس نے آپ کو چھوا؟\n",
            "3      its part of life         اس کی زندگی کا حصہ\n",
            "4        zain isnt ugly        زین بدصورت نہیں ہے۔\n",
            "\n",
            "--- Dataset Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 24524 entries, 0 to 24523\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   english  24524 non-null  object\n",
            " 1   urdu     24524 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 383.3+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "english_file_path = 'english.txt'\n",
        "urdu_file_path = 'urdu.txt'\n",
        "\n",
        "# --- Function to Load and Preprocess Data with Debugging ---\n",
        "def load_parallel_corpus_debug(eng_path, urd_path):\n",
        "    english_sentences = []\n",
        "    urdu_sentences = []\n",
        "    line_num = 0\n",
        "    max_lines = 0 # To store the length of the longer file\n",
        "\n",
        "    if not os.path.exists(eng_path):\n",
        "        print(f\"Error: English file not found at '{eng_path}'\")\n",
        "        return None\n",
        "    if not os.path.exists(urd_path):\n",
        "        print(f\"Error: Urdu file not found at '{urd_path}'\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        print(f\"Reading and comparing lines from '{eng_path}' and '{urd_path}'...\")\n",
        "        with open(eng_path, 'r', encoding='utf-8') as f_eng, \\\n",
        "             open(urd_path, 'r', encoding='utf-8') as f_urd:\n",
        "\n",
        "            while True:\n",
        "                line_num += 1\n",
        "                eng_line_raw = f_eng.readline()\n",
        "                urd_line_raw = f_urd.readline()\n",
        "\n",
        "                # --- End of File Handling ---\n",
        "                if not eng_line_raw and not urd_line_raw:\n",
        "                    print(f\"Both files finished. Processed {line_num - 1} lines.\")\n",
        "                    max_lines = line_num -1\n",
        "                    break\n",
        "                if not eng_line_raw:\n",
        "                    print(f\"English file ended at line {line_num - 1}, but Urdu file continues.\")\n",
        "                    print(f\"  Extra Urdu line {line_num}: {urd_line_raw.strip()!r}\")\n",
        "                    # Decide how to handle: For now, we'll break, leading to mismatch\n",
        "                    max_lines = line_num # Urdu file is longer\n",
        "                    # Read rest of Urdu file to get its total length (optional)\n",
        "                    urd_lines_count = line_num\n",
        "                    while f_urd.readline():\n",
        "                        urd_lines_count += 1\n",
        "                    max_lines = urd_lines_count\n",
        "                    break\n",
        "                if not urd_line_raw:\n",
        "                    print(f\"Urdu file ended at line {line_num - 1}, but English file continues.\")\n",
        "                    print(f\"  Extra English line {line_num}: {eng_line_raw.strip()!r}\")\n",
        "                    # Decide how to handle: For now, we'll break, leading to mismatch\n",
        "                    max_lines = line_num # English file is longer\n",
        "                     # Read rest of English file to get its total length (optional)\n",
        "                    eng_lines_count = line_num\n",
        "                    while f_eng.readline():\n",
        "                        eng_lines_count += 1\n",
        "                    max_lines = eng_lines_count\n",
        "                    break\n",
        "\n",
        "                # --- Process Lines ---\n",
        "                eng_sentence = eng_line_raw.strip()\n",
        "                urd_sentence = urd_line_raw.strip()\n",
        "\n",
        "                is_eng_valid = bool(eng_sentence)\n",
        "                is_urd_valid = bool(urd_sentence)\n",
        "\n",
        "                # Check if validity (non-empty after strip) differs\n",
        "                if is_eng_valid != is_urd_valid:\n",
        "                    print(f\"WARNING: Mismatch in validity at line {line_num}:\")\n",
        "                    print(f\"  English Raw: {eng_line_raw!r} -> Stripped: {eng_sentence!r} (Valid: {is_eng_valid})\")\n",
        "                    print(f\"  Urdu Raw   : {urd_line_raw!r} -> Stripped: {urd_sentence!r} (Valid: {is_urd_valid})\")\n",
        "                    # If you want to enforce skipping based on mismatch:\n",
        "                    # continue # This would skip both lines if one is invalid\n",
        "\n",
        "                # Append if valid (original logic)\n",
        "                if is_eng_valid:\n",
        "                    english_sentences.append(eng_sentence)\n",
        "                if is_urd_valid:\n",
        "                    urdu_sentences.append(urd_sentence)\n",
        "\n",
        "        print(\"-\" * 20)\n",
        "        print(f\"Finished reading. Raw lines processed/compared: {max_lines}\")\n",
        "        print(f\"Filtered English sentences collected: {len(english_sentences)}\")\n",
        "        print(f\"Filtered Urdu sentences collected: {len(urdu_sentences)}\")\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "        # --- Validation ---\n",
        "        if len(english_sentences) != len(urdu_sentences):\n",
        "            print(f\"Error: Final sentence counts mismatch after filtering!\")\n",
        "            # No need to raise error here, just report and return None\n",
        "            return None\n",
        "\n",
        "        # --- Create DataFrame ---\n",
        "        df = pd.DataFrame({\n",
        "            'english': english_sentences,\n",
        "            'urdu': urdu_sentences\n",
        "        })\n",
        "        print(f\"Successfully loaded {len(df)} aligned sentence pairs.\")\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during processing: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Load the Data ---\n",
        "parallel_df = load_parallel_corpus_debug(english_file_path, urdu_file_path)\n",
        "\n",
        "# --- Display Sample Data ---\n",
        "if parallel_df is not None:\n",
        "    print(\"\\n--- Sample Data (First 5 pairs) ---\")\n",
        "    print(parallel_df.head())\n",
        "    print(\"\\n--- Dataset Info ---\")\n",
        "    print(parallel_df.info())\n",
        "else:\n",
        "    print(\"\\nFailed to load the dataset due to mismatches or errors.\")\n",
        "    print(\"Please check the WARNINGS/ERRORS above and inspect the data files.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6a4H7HvYZ55",
        "outputId": "53e370a8-9a18-4e17-ef94-057027d2ea57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading and comparing lines from 'english.txt' and 'urdu.txt'...\n",
            "Both files finished. Processed 24524 lines.\n",
            "--------------------\n",
            "Finished reading. Raw lines processed/compared: 24524\n",
            "Filtered English sentences collected: 24524\n",
            "Filtered Urdu sentences collected: 24524\n",
            "--------------------\n",
            "Successfully loaded 24524 aligned sentence pairs.\n",
            "\n",
            "--- Sample Data (First 5 pairs) ---\n",
            "                english                       urdu\n",
            "0   is zain your nephew      زین تمہارا بھتیجا ہے۔\n",
            "1  i wish youd trust me  کاش تم مجھ پر بھروسہ کرتے\n",
            "2      did he touch you      کیا اس نے آپ کو چھوا؟\n",
            "3      its part of life         اس کی زندگی کا حصہ\n",
            "4        zain isnt ugly        زین بدصورت نہیں ہے۔\n",
            "\n",
            "--- Dataset Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 24524 entries, 0 to 24523\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   english  24524 non-null  object\n",
            " 1   urdu     24524 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 383.3+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "english_file_path = 'english.txt'\n",
        "urdu_file_path = 'urdu.txt'\n",
        "\n",
        "# --- Function to Load and Preprocess Data with Debugging ---\n",
        "def load_parallel_corpus_debug(eng_path, urd_path):\n",
        "    english_sentences = []\n",
        "    urdu_sentences = []\n",
        "    line_num = 0\n",
        "    max_lines = 0 # To store the length of the longer file\n",
        "\n",
        "    if not os.path.exists(eng_path):\n",
        "        print(f\"Error: English file not found at '{eng_path}'\")\n",
        "        return None\n",
        "    if not os.path.exists(urd_path):\n",
        "        print(f\"Error: Urdu file not found at '{urd_path}'\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        print(f\"Reading and comparing lines from '{eng_path}' and '{urd_path}'...\")\n",
        "        with open(eng_path, 'r', encoding='utf-8') as f_eng, \\\n",
        "             open(urd_path, 'r', encoding='utf-8') as f_urd:\n",
        "\n",
        "            while True:\n",
        "                line_num += 1\n",
        "                eng_line_raw = f_eng.readline()\n",
        "                urd_line_raw = f_urd.readline()\n",
        "\n",
        "                # --- End of File Handling ---\n",
        "                if not eng_line_raw and not urd_line_raw:\n",
        "                    print(f\"Both files finished. Processed {line_num - 1} lines.\")\n",
        "                    max_lines = line_num -1\n",
        "                    break\n",
        "                if not eng_line_raw:\n",
        "                    print(f\"English file ended at line {line_num - 1}, but Urdu file continues.\")\n",
        "                    print(f\"  Extra Urdu line {line_num}: {urd_line_raw.strip()!r}\")\n",
        "                    # Decide how to handle: For now, we'll break, leading to mismatch\n",
        "                    max_lines = line_num # Urdu file is longer\n",
        "                    # Read rest of Urdu file to get its total length (optional)\n",
        "                    urd_lines_count = line_num\n",
        "                    while f_urd.readline():\n",
        "                        urd_lines_count += 1\n",
        "                    max_lines = urd_lines_count\n",
        "                    break\n",
        "                if not urd_line_raw:\n",
        "                    print(f\"Urdu file ended at line {line_num - 1}, but English file continues.\")\n",
        "                    print(f\"  Extra English line {line_num}: {eng_line_raw.strip()!r}\")\n",
        "                    # Decide how to handle: For now, we'll break, leading to mismatch\n",
        "                    max_lines = line_num # English file is longer\n",
        "                     # Read rest of English file to get its total length (optional)\n",
        "                    eng_lines_count = line_num\n",
        "                    while f_eng.readline():\n",
        "                        eng_lines_count += 1\n",
        "                    max_lines = eng_lines_count\n",
        "                    break\n",
        "\n",
        "                # --- Process Lines ---\n",
        "                eng_sentence = eng_line_raw.strip()\n",
        "                urd_sentence = urd_line_raw.strip()\n",
        "\n",
        "                is_eng_valid = bool(eng_sentence)\n",
        "                is_urd_valid = bool(urd_sentence)\n",
        "\n",
        "                # Check if validity (non-empty after strip) differs\n",
        "                if is_eng_valid != is_urd_valid:\n",
        "                    print(f\"WARNING: Mismatch in validity at line {line_num}:\")\n",
        "                    print(f\"  English Raw: {eng_line_raw!r} -> Stripped: {eng_sentence!r} (Valid: {is_eng_valid})\")\n",
        "                    print(f\"  Urdu Raw   : {urd_line_raw!r} -> Stripped: {urd_sentence!r} (Valid: {is_urd_valid})\")\n",
        "                    # If you want to enforce skipping based on mismatch:\n",
        "                    # continue # This would skip both lines if one is invalid\n",
        "\n",
        "                # Append if valid (original logic)\n",
        "                if is_eng_valid:\n",
        "                    english_sentences.append(eng_sentence)\n",
        "                if is_urd_valid:\n",
        "                    urdu_sentences.append(urd_sentence)\n",
        "\n",
        "        print(\"-\" * 20)\n",
        "        print(f\"Finished reading. Raw lines processed/compared: {max_lines}\")\n",
        "        print(f\"Filtered English sentences collected: {len(english_sentences)}\")\n",
        "        print(f\"Filtered Urdu sentences collected: {len(urdu_sentences)}\")\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "        # --- Validation ---\n",
        "        if len(english_sentences) != len(urdu_sentences):\n",
        "            print(f\"Error: Final sentence counts mismatch after filtering!\")\n",
        "            # No need to raise error here, just report and return None\n",
        "            return None\n",
        "\n",
        "        # --- Create DataFrame ---\n",
        "        df = pd.DataFrame({\n",
        "            'english': english_sentences,\n",
        "            'urdu': urdu_sentences\n",
        "        })\n",
        "        print(f\"Successfully loaded {len(df)} aligned sentence pairs.\")\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during processing: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Load the Data ---\n",
        "parallel_df = load_parallel_corpus_debug(english_file_path, urdu_file_path)\n",
        "\n",
        "# --- Display Sample Data ---\n",
        "if parallel_df is not None:\n",
        "    print(\"\\n--- Sample Data (First 5 pairs) ---\")\n",
        "    print(parallel_df.head())\n",
        "    print(\"\\n--- Dataset Info ---\")\n",
        "    print(parallel_df.info())\n",
        "else:\n",
        "    print(\"\\nFailed to load the dataset due to mismatches or errors.\")\n",
        "    print(\"Please check the WARNINGS/ERRORS above and inspect the data files.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "mn5pnhasZGOF",
        "outputId": "bd76b68d-07f8-4920-ed7b-70e175b1bed9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>english</th>\n",
              "      <th>urdu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>is zain your nephew</td>\n",
              "      <td>زین تمہارا بھتیجا ہے۔</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i wish youd trust me</td>\n",
              "      <td>کاش تم مجھ پر بھروسہ کرتے</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>did he touch you</td>\n",
              "      <td>کیا اس نے آپ کو چھوا؟</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>its part of life</td>\n",
              "      <td>اس کی زندگی کا حصہ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>zain isnt ugly</td>\n",
              "      <td>زین بدصورت نہیں ہے۔</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24519</th>\n",
              "      <td>i am in a hurry today</td>\n",
              "      <td>میں آج جلدی میں ہوں۔</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24520</th>\n",
              "      <td>take this medicine</td>\n",
              "      <td>یہ دوا لے لو</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24521</th>\n",
              "      <td>this is the case</td>\n",
              "      <td>یہ معاملہ ہے</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24522</th>\n",
              "      <td>zains tipsy</td>\n",
              "      <td>زین ٹپسی</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24523</th>\n",
              "      <td>i am not angry</td>\n",
              "      <td>میں ناراض نہیں ہوں</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24524 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                     english                       urdu\n",
              "0        is zain your nephew      زین تمہارا بھتیجا ہے۔\n",
              "1       i wish youd trust me  کاش تم مجھ پر بھروسہ کرتے\n",
              "2           did he touch you      کیا اس نے آپ کو چھوا؟\n",
              "3           its part of life         اس کی زندگی کا حصہ\n",
              "4             zain isnt ugly        زین بدصورت نہیں ہے۔\n",
              "...                      ...                        ...\n",
              "24519  i am in a hurry today       میں آج جلدی میں ہوں۔\n",
              "24520     take this medicine               یہ دوا لے لو\n",
              "24521       this is the case               یہ معاملہ ہے\n",
              "24522            zains tipsy                   زین ٹپسی\n",
              "24523         i am not angry         میں ناراض نہیں ہوں\n",
              "\n",
              "[24524 rows x 2 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parallel_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aRtpYVVaxB_",
        "outputId": "90083ba7-d7eb-40f6-86be-0a9a4942b415"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: ہیلو ، آپ کیسے ہیں؟\n",
            "Urdu Back-Translation: ہیلو ، آپ کیسے ہیں؟\n",
            "Urdu Translation: Hello, how are you?\n"
          ]
        }
      ],
      "source": [
        "from BackTranslation import BackTranslation\n",
        "\n",
        "# Initialize translator (create only one instance)\n",
        "trans = BackTranslation()\n",
        "\n",
        "result = trans.translate(\"ہیلو ، آپ کیسے ہیں؟\", src=\"ur\", tmp=\"en\")\n",
        "print(f\"Original: {result.source_text}\")\n",
        "print(f\"Urdu Back-Translation: {result.result_text}\")\n",
        "print(f\"Urdu Translation: {result.tran_text}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RDzIeIslaN7",
        "outputId": "2d8f24ca-f8e2-45fb-c641-1d794ffd0e17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading repository zip file...\n",
            "Download successful!\n",
            "Text folder has been extracted to: text\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# URL for the repository's master branch as a zip file\n",
        "zip_url = \"https://github.com/zeerakahmed/makhzan/archive/refs/heads/master.zip\"\n",
        "\n",
        "# Download the zip file\n",
        "print(\"Downloading repository zip file...\")\n",
        "response = requests.get(zip_url)\n",
        "if response.status_code == 200:\n",
        "    print(\"Download successful!\")\n",
        "    # Load the content into a BytesIO object\n",
        "    zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
        "\n",
        "    # Define a temporary extraction directory\n",
        "    temp_dir = \"makhzan_temp\"\n",
        "\n",
        "    # Extract the zip file content\n",
        "    zip_file.extractall(temp_dir)\n",
        "\n",
        "    # Path to the text folder within the extracted directory\n",
        "    extracted_folder = os.path.join(temp_dir, \"makhzan-master\", \"text\")\n",
        "\n",
        "    # Define destination directory for the text folder\n",
        "    destination = \"text\"\n",
        "\n",
        "    # Check if the text folder exists and move it to destination\n",
        "    if os.path.exists(extracted_folder):\n",
        "        # Remove the destination directory if it already exists\n",
        "        if os.path.exists(destination):\n",
        "            shutil.rmtree(destination)\n",
        "        shutil.move(extracted_folder, destination)\n",
        "        print(f\"Text folder has been extracted to: {destination}\")\n",
        "    else:\n",
        "        print(\"Text folder not found in the repository.\")\n",
        "\n",
        "    # Optionally, clean up the temporary directory\n",
        "    shutil.rmtree(temp_dir)\n",
        "else:\n",
        "    print(\"Failed to download the repository zip file. Status code:\", response.status_code)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "5cdc0fb17be645c88ce20e9271735097",
            "f1aa841e84144e43a7ad2465c2887e01",
            "4f0c5d43aa414bcb99537c0190cf2d42",
            "dad14fb2851d4925913fbdf9468d73fe",
            "8abf3e630b564b29aea4c60760186974",
            "616f3517057d4c038e257d7323d60351",
            "dad27f23524348eb8c2b800f3979178a",
            "02001e02f7474e36a2d5f1ebf2002b75",
            "82eca231657c4e1ca4d5d0a53cdd97a4",
            "d0a939bcd94646ffa4e6d588b283196d",
            "72f69738faf54f788d43d61f8b98f3f6"
          ]
        },
        "id": "P8AuezljlFGc",
        "outputId": "b39faa85-f89e-4cd5-8e37-d7b35e260a7f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/anaconda3/envs/deep_project/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting XML processing in directory: text\n",
            "Found 6315 XML files matching the pattern.\n",
            "Initialized pysbd sentence segmenter for Urdu.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing XML Files:   0%|          | 0/6315 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing XML Files: 100%|██████████| 6315/6315 [01:05<00:00, 96.81it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Processing Summary ---\n",
            "Total files attempted: 6315\n",
            "Files successfully processed (found body & no errors): 6315\n",
            "Total Urdu sentences extracted: 215343\n",
            "\n",
            "--- Sample Extracted Sentences (First 10) ---\n",
            "1: بنگلہ دیش کی عدالتِ عالیہ نے طلاق کے ایک مقدمے کا فیصلہ کرتے ہوئے علما کے فتووں کو غیر قانونی قرار دیا ہے۔\n",
            "2: عدالت نے پارلیمنٹ سے یہ درخواست کی ہے کہ وہ جلد ایسا قانون وضع کرے کہ جس کے بعد فتویٰ بازی قابلِ دست اندازیِ پولیس جرم بن جائے۔\n",
            "3: بنگلہ دیش کے علما نے اس فیصلے پر بھر پور ردِ عمل ظاہرکرتے ہوئے اس کے خلاف ملک گیر تحریک چلانے کا اعلان کیا ہے۔\n",
            "4: اس ضمن میں علما کی ایک تنظیم ”اسلامک یونٹی الائنس“ نے متعلقہ ججوں کو مرتد یعنی دین سے منحرف اور دائرۂ اسلام سے خارج قرار دیا ہے۔\n",
            "5: فتوے کا لفظ دو موقعوں پر استعمال ہوتا ہے۔\n",
            "6: ایک اس موقع پر جب کوئی صاحبِ علم شریعت کے کسی مئلے کے بارے میں اپنی رائے پیش کرتا ہے۔\n",
            "7: دوسرے اس موقع پر جب کوئی عالمِ دین کسی خاص واقعے کے حوالے سے اپنا قانونی فیصلہ صادر کرتا ہے۔\n",
            "8: ایک عرصے سے ہمارے علما کے ہاں اس دوسرے موقعِ استعمال کا غلبہ ہو گیا ہے۔\n",
            "9: اس کا نتیجہ یہ نکلا ہے کہ اس لفظ کا رائے یا نقطۂ نظر کے مفہوم میں استعمال کم و بیش متروک ہو گیا ہے۔\n",
            "10: چنانچہ اب فتوے کا مطلب ہی علما کی طرف سے کسی خاص مألے یا واقعے کے بارے میں حتمی فیصلے کا صدور سمجھا جاتا ہے۔\n",
            "\n",
            "--- Sample Extracted Sentences (Last 10) ---\n",
            "215334: رکوع وسجود کی تسبیحات: رکوع وسجود کے موقع پر بھی متعدد تسبیحات اور دعائیں رسول اللہ صلی اللہ علیہ وسلم سے منقول ہیں اور نمازی ان میں سے کوئی بھی منتخب کر سکتا ہے۔\n",
            "215335: تشہد کے کلمات: رسول اللہ صلی اللہ علیہ وسلم نے مختلف صحابہ کو تشہد کے مختلف کلمات کی تعلیم دی۔\n",
            "215336: آدمی ان میں سے جو کلمات بھی چاہے، پڑھ سکتا ہے۔\n",
            "215337: تشہد کے بعد کی دعائیں: رسول اللہ صلی اللہ علیہ وسلم نے فرمایا کہ تشہد کے بعد آدمی اپنی پسند کی جو بھی دعا چاہے، پڑھ سکتا ہے۔\n",
            "215338: ۱؂ مسلم، رقم ۹۷۱ ۲؂ مسلم، رقم ۱۲۱۸۔\n",
            "215339: ۳؂ نسائی، رقم ۹۶۹۔\n",
            "215340: ۴؂ ابو د اؤد، رقم ۸۸۳۔\n",
            "215341: ۵؂ ابو داؤد، رقم ۸۸۴۔\n",
            "215342: ۶؂ ابو داؤد، رقم ۸۷۱۔\n",
            "215343: ۸۷۳۔\n",
            "\n",
            "Saving extracted sentences to extracted_urdu_monolingual_pysbd.txt...\n",
            "Sentences saved successfully to extracted_urdu_monolingual_pysbd.txt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from lxml import etree\n",
        "from tqdm.auto import tqdm\n",
        "import re\n",
        "import sys\n",
        "import pysbd # Using pysbd\n",
        "\n",
        "# --- Configuration ---\n",
        "# Ensure this path is correct for your Colab environment where you uploaded/mounted the data\n",
        "XML_DIR = \"text\"\n",
        "\n",
        "# --- Function to extract text from relevant XML elements ---\n",
        "def extract_text_from_element(element):\n",
        "    \"\"\"Extracts text using itertext.\"\"\"\n",
        "    text_parts = []\n",
        "    # Using itertext() gets text nodes directly, including those between tags\n",
        "    for text in element.itertext():\n",
        "         cleaned_text = text.strip()\n",
        "         # Basic cleaning: replace multiple whitespace chars with a single space\n",
        "         cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "         if cleaned_text:\n",
        "             text_parts.append(cleaned_text)\n",
        "    # Join parts with a single space\n",
        "    return \" \".join(text_parts)\n",
        "\n",
        "# --- Main Parsing and Sentence Extraction Logic ---\n",
        "all_urdu_sentences = []\n",
        "processed_files = 0\n",
        "failed_files = []\n",
        "\n",
        "print(f\"Starting XML processing in directory: {XML_DIR}\")\n",
        "\n",
        "if not os.path.isdir(XML_DIR):\n",
        "    print(f\"Error: Directory not found - {XML_DIR}\")\n",
        "    # Stop execution if directory isn't found\n",
        "    sys.exit(1) # Or raise an error depending on preference\n",
        "\n",
        "try:\n",
        "    # List files and sort numerically based on the number in the filename\n",
        "    xml_files = sorted(\n",
        "        [f for f in os.listdir(XML_DIR) if f.endswith('.xml') and f[:-4].isdigit()],\n",
        "        key=lambda x: int(x[:-4]) # Assumes filenames are like '0001.xml'\n",
        "    )\n",
        "    print(f\"Found {len(xml_files)} XML files matching the pattern.\")\n",
        "except FileNotFoundError:\n",
        "     print(f\"Error: Cannot list files in directory - {XML_DIR}. Check the path.\")\n",
        "     xml_files = []\n",
        "except Exception as e:\n",
        "     print(f\"An unexpected error occurred while listing files: {e}\")\n",
        "     xml_files = []\n",
        "\n",
        "# Initialize pysbd segmenter for Urdu\n",
        "# clean=False preserves some potentially meaningful structures if needed later,\n",
        "# clean=True performs more aggressive cleaning (like removing periods within numbers)\n",
        "try:\n",
        "    seg = pysbd.Segmenter(language=\"ur\", clean=False)\n",
        "    print(\"Initialized pysbd sentence segmenter for Urdu.\")\n",
        "except Exception as e:\n",
        "     print(f\"Error initializing pysbd: {e}\")\n",
        "     print(\"Make sure 'pysbd' is installed correctly.\")\n",
        "     sys.exit(1) # Stop if segmenter fails\n",
        "\n",
        "\n",
        "for filename in tqdm(xml_files, desc=\"Processing XML Files\"):\n",
        "    filepath = os.path.join(XML_DIR, filename)\n",
        "    try:\n",
        "        # Parse XML robustly\n",
        "        parser = etree.XMLParser(recover=True, encoding='utf-8') # recover=True helps with minor errors\n",
        "        tree = etree.parse(filepath, parser=parser)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        # Find the <body> element using XPath\n",
        "        body = root.find('.//body')\n",
        "\n",
        "        if body is not None:\n",
        "            file_text_parts = []\n",
        "            # Iterate through paragraph tags within the body\n",
        "            for p_element in body.xpath('.//p'):\n",
        "                # Extract text using the helper function (which uses itertext)\n",
        "                paragraph_text = extract_text_from_element(p_element)\n",
        "\n",
        "                # Skip paragraphs that seem empty after extraction or likely just foreign annotations\n",
        "                # Check if paragraph contains an Arabic annotation AND its own direct text is empty/whitespace\n",
        "                is_only_foreign_annotation = p_element.xpath('.//annotation[@lang=\"ar\"]') and not ''.join(p_element.xpath('./text()')).strip()\n",
        "\n",
        "                if paragraph_text and not is_only_foreign_annotation:\n",
        "                     # Text already cleaned in extract_text_from_element\n",
        "                     file_text_parts.append(paragraph_text)\n",
        "\n",
        "            # Join paragraphs with a space (sent_tokenize handles sentence breaks)\n",
        "            full_doc_text = \" \".join(file_text_parts)\n",
        "\n",
        "            if full_doc_text:\n",
        "                # --- Use pysbd for Sentence Segmentation ---\n",
        "                sentences = seg.segment(full_doc_text)\n",
        "                # -----------------------------------------\n",
        "                for sent in sentences:\n",
        "                    # Final clean: strip leading/trailing whitespace from sentence\n",
        "                    cleaned_sent = sent.strip()\n",
        "                    # Basic filter: ensure sentence is not empty and maybe has minimum length\n",
        "                    if cleaned_sent and len(cleaned_sent) > 3: # Example: min length 4 chars\n",
        "                        all_urdu_sentences.append(cleaned_sent)\n",
        "            processed_files += 1\n",
        "        else:\n",
        "             # Optionally track files without a body tag if needed\n",
        "             # print(f\"Warning: No <body> tag found in {filename}.\")\n",
        "             pass # Silently skip\n",
        "\n",
        "    except etree.XMLSyntaxError as e:\n",
        "        # More specific error for bad XML\n",
        "        print(f\"\\nXML Syntax Error processing {filename}: {e}\")\n",
        "        failed_files.append(filename + \" (XML Syntax Error)\")\n",
        "    except Exception as e:\n",
        "        # General catch-all for other errors (pysbd, file reading, etc.)\n",
        "        print(f\"\\nAn unexpected error occurred processing {filename}: {e}\")\n",
        "        # Include exception type for better debugging\n",
        "        failed_files.append(filename + f\" ({type(e).__name__}: {e})\")\n",
        "\n",
        "\n",
        "# --- Final Summary ---\n",
        "print(\"\\n--- Processing Summary ---\")\n",
        "print(f\"Total files attempted: {len(xml_files)}\")\n",
        "print(f\"Files successfully processed (found body & no errors): {processed_files}\")\n",
        "print(f\"Total Urdu sentences extracted: {len(all_urdu_sentences)}\")\n",
        "if failed_files:\n",
        "    print(f\"Files failed or skipped due to errors: {len(failed_files)}\")\n",
        "    # Show first few failed files for debugging\n",
        "    # print(\"Sample failed/skipped files:\")\n",
        "    # for f_detail in failed_files[:5]:\n",
        "    #     print(f\"  - {f_detail}\")\n",
        "\n",
        "\n",
        "# --- Display Sample Sentences ---\n",
        "print(\"\\n--- Sample Extracted Sentences (First 10) ---\")\n",
        "if all_urdu_sentences:\n",
        "    for i, sent in enumerate(all_urdu_sentences[:10]):\n",
        "        print(f\"{i+1}: {sent}\")\n",
        "else:\n",
        "    print(\"No sentences were extracted.\")\n",
        "\n",
        "print(\"\\n--- Sample Extracted Sentences (Last 10) ---\")\n",
        "if len(all_urdu_sentences) >= 10:\n",
        "    for i, sent in enumerate(all_urdu_sentences[-10:]):\n",
        "        # Correct index calculation for last 10\n",
        "        print(f\"{len(all_urdu_sentences) - 10 + i + 1}: {sent}\")\n",
        "elif all_urdu_sentences:\n",
        "     print(\"(Showing all extracted sentences as there are less than 10)\")\n",
        "     for i, sent in enumerate(all_urdu_sentences):\n",
        "        print(f\"{i+1}: {sent}\")\n",
        "else:\n",
        "    print(\"No sentences were extracted.\")\n",
        "\n",
        "\n",
        "# --- Save sentences to a file ---\n",
        "output_file = \"extracted_urdu_monolingual_pysbd.txt\"\n",
        "print(f\"\\nSaving extracted sentences to {output_file}...\")\n",
        "try:\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        for sent in all_urdu_sentences:\n",
        "            f.write(sent + \"\\n\")\n",
        "    print(f\"Sentences saved successfully to {output_file}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving sentences: {e}\")\n",
        "\n",
        "# You can now use the file 'extracted_urdu_monolingual_pysbd.txt' for back-translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Code you have to focus on:\n",
        "import pandas as pd\n",
        "import os\n",
        "from BackTranslation import BackTranslation\n",
        "from tqdm.auto import tqdm # For progress bar\n",
        "import time # To add delays if needed\n",
        "import concurrent.futures\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import random # <-- Added for sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "urdu_monolingual_file = \"extracted_urdu_monolingual_pysbd.txt\"\n",
        "output_csv_file = \"urdu_english_backtranslated_parallel_sampled.csv\" # Changed output filename slightly\n",
        "MAX_WORKERS = 4 # <<<--- ADJUSTED: Limit to 2 concurrent translation threads\n",
        "SAMPLE_SIZE = 10000 # <<<--- ADDED: Number of sentences to sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Urdu sentences from: extracted_urdu_monolingual_pysbd.txt\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Load Urdu Sentences ---\n",
        "print(f\"Loading Urdu sentences from: {urdu_monolingual_file}\")\n",
        "if not os.path.exists(urdu_monolingual_file):\n",
        "    print(f\"Error: Input file not found at '{urdu_monolingual_file}'\")\n",
        "    exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully loaded 215343 non-empty Urdu sentences.\n"
          ]
        }
      ],
      "source": [
        "urdu_sentences_from_file_full = [] # Load into a temporary full list first\n",
        "try:\n",
        "    with open(urdu_monolingual_file, 'r', encoding='utf-8') as f:\n",
        "        urdu_sentences_from_file_full = [line.strip() for line in f if line.strip()]\n",
        "    print(f\"Successfully loaded {len(urdu_sentences_from_file_full)} non-empty Urdu sentences.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error reading file {urdu_monolingual_file}: {e}\")\n",
        "    exit()\n",
        "\n",
        "if not urdu_sentences_from_file_full:\n",
        "    print(\"No sentences found in the input file. Exiting.\")\n",
        "    exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sampling 10000 sentences from the loaded 215343 sentences...\n",
            "Proceeding with 10000 sampled sentences.\n"
          ]
        }
      ],
      "source": [
        "# --- ADDED: Sampling Logic ---\n",
        "if len(urdu_sentences_from_file_full) > SAMPLE_SIZE:\n",
        "    print(f\"Sampling {SAMPLE_SIZE} sentences from the loaded {len(urdu_sentences_from_file_full)} sentences...\")\n",
        "    random.seed(42) # Optional: for reproducible sampling\n",
        "    urdu_sentences_to_process = random.sample(urdu_sentences_from_file_full, SAMPLE_SIZE)\n",
        "    print(f\"Proceeding with {len(urdu_sentences_to_process)} sampled sentences.\")\n",
        "else:\n",
        "    print(f\"Number of sentences ({len(urdu_sentences_from_file_full)}) is less than or equal to SAMPLE_SIZE ({SAMPLE_SIZE}). Processing all loaded sentences.\")\n",
        "    urdu_sentences_to_process = urdu_sentences_from_file_full\n",
        "# --- End of Sampling Logic ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing BackTranslation object...\n",
            "BackTranslation initialized.\n"
          ]
        }
      ],
      "source": [
        "# --- 2. Initialize BackTranslator ---\n",
        "# Initialize *once* outside the loop/worker function\n",
        "print(\"Initializing BackTranslation object...\")\n",
        "try:\n",
        "    # Assuming BackTranslation() is thread-safe for concurrent calls\n",
        "    trans = BackTranslation()\n",
        "    print(\"BackTranslation initialized.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing BackTranslation: {e}\")\n",
        "    print(\"Please ensure the 'BackTranslation' library and its dependencies are installed correctly.\")\n",
        "    exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 3. Define Worker Function ---\n",
        "def translate_sentence(sentence):\n",
        "    \"\"\"\n",
        "    Worker function to translate a single sentence.\n",
        "    Handles individual translation errors.\n",
        "    Returns a tuple: (original_sentence, english_translation_or_None)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Perform the translation: Urdu -> English (tmp) -> Urdu\n",
        "        result = trans.translate(sentence, src='ur', tmp='en')\n",
        "        english_translation = result.tran_text\n",
        "\n",
        "        if english_translation and english_translation.strip():\n",
        "            return sentence, english_translation.strip()\n",
        "        else:\n",
        "            # print(f\"\\nWarning: Received empty/invalid translation for: {sentence!r}\") # Too noisy in parallel\n",
        "            return sentence, None # Indicate failure for this sentence\n",
        "    except Exception as e:\n",
        "        # Add a small delay before retrying or failing, might help with transient rate limits\n",
        "        # time.sleep(0.5) # Optional: uncomment if needed, but slows down process\n",
        "        # print(f\"\\nError translating sentence: {sentence!r}. Error: {e}\") # Too noisy in parallel\n",
        "        return sentence, None # Indicate failure for this sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting parallel back-translation for 10000 sentences...\n",
            "Using up to 4 concurrent workers.\n",
            "Appending results incrementally to: urdu_english_backtranslated_parallel_sampled.csv\n",
            "Output file is new or empty. Writing header row.\n",
            "Processing and appending translations...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating:   2%|▏         | 206/10000 [1:40:10<79:22:23, 29.18s/it]\n"
          ]
        }
      ],
      "source": [
        "# CODE YOU HAVE TO FOCUS ON:\n",
        "import csv # <-- Import the csv module\n",
        "import os  # <-- Make sure os is imported (likely already is)\n",
        "\n",
        "# --- (Previous code for loading, sampling, initializing BackTranslation, and defining translate_sentence remains the same) ---\n",
        "\n",
        "# --- Configuration (ensure output_csv_file is defined) ---\n",
        "# output_csv_file = \"urdu_english_backtranslated_parallel_sampled.csv\" # Should be defined earlier\n",
        "\n",
        "# --- 4-6. Iterate, Translate (in Parallel), Store (Incrementally), and Handle Errors ---\n",
        "successful_translations = 0 # Counter for successful translations\n",
        "failed_translations = 0     # Counter for failed translations\n",
        "\n",
        "print(f\"\\nStarting parallel back-translation for {len(urdu_sentences_to_process)} sentences...\") # Use the sampled list size\n",
        "print(f\"Using up to {MAX_WORKERS} concurrent workers.\") # Now reflects 2\n",
        "print(f\"Appending results incrementally to: {output_csv_file}\")\n",
        "\n",
        "# Check if the file exists to determine if header is needed\n",
        "file_exists = os.path.exists(output_csv_file)\n",
        "# Check if file exists *and* has content (more robust than just exists)\n",
        "is_empty = not file_exists or os.path.getsize(output_csv_file) == 0\n",
        "\n",
        "# Use ThreadPoolExecutor for I/O-bound tasks\n",
        "# Open the CSV file *before* the executor starts and keep it open\n",
        "try:\n",
        "    with open(output_csv_file, 'a', newline='', encoding='utf-8') as csvfile, \\\n",
        "         ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor: # MAX_WORKERS is now 2\n",
        "\n",
        "        csv_writer = csv.writer(csvfile)\n",
        "\n",
        "        # Write header only if the file is new/empty\n",
        "        if is_empty:\n",
        "            csv_writer.writerow(['urdu', 'english'])\n",
        "            print(\"Output file is new or empty. Writing header row.\")\n",
        "\n",
        "        # Use executor.map to apply the worker function to each sentence in the sampled list\n",
        "        futures = executor.map(translate_sentence, urdu_sentences_to_process) # Use the sampled list\n",
        "\n",
        "        # Process results as they complete using tqdm for progress\n",
        "        # Iterate directly over the futures iterator wrapped in tqdm\n",
        "        print(\"Processing and appending translations...\")\n",
        "        for original, translation in tqdm(futures, total=len(urdu_sentences_to_process), desc=\"Translating\"):\n",
        "            if translation is not None:\n",
        "                # Write the successful translation pair directly to the CSV\n",
        "                csv_writer.writerow([original, translation])\n",
        "                successful_translations += 1\n",
        "            else:\n",
        "                failed_translations += 1\n",
        "        # The 'with open...' block ensures the file is flushed and closed here\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during the translation process: {e}\")\n",
        "    print(\"Progress up to this point might be saved in the CSV file.\")\n",
        "    # Depending on the error, you might want to exit or handle differently\n",
        "    # exit() # Optional: stop execution if a critical error occurs during processing\n",
        "\n",
        "# --- Post-Processing Summary ---\n",
        "print(\"\\n--- Translation Summary ---\")\n",
        "print(f\"Successfully translated pairs appended to CSV: {successful_translations}\")\n",
        "print(f\"Sentences failed during translation: {failed_translations}\")\n",
        "print(f\"Total sentences attempted (sampled): {len(urdu_sentences_to_process)}\") # Report based on sampled size\n",
        "\n",
        "# --- 7 & 8 Removed: DataFrame creation and final save are no longer needed ---\n",
        "# No need to create the full DataFrame in memory or save it again.\n",
        "\n",
        "if successful_translations == 0 and failed_translations > 0:\n",
        "     print(\"\\nNo sentences were successfully translated and saved.\")\n",
        "     print(\"Check for persistent errors like API rate limits, network issues, or configuration problems.\")\n",
        "elif successful_translations == 0 and failed_translations == 0:\n",
        "     print(\"\\nNo sentences were processed (input list might have been empty).\")\n",
        "else:\n",
        "     print(f\"\\nData has been incrementally saved to: {output_csv_file}\")\n",
        "\n",
        "\n",
        "print(\"\\nProcessing finished.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "deep_project",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02001e02f7474e36a2d5f1ebf2002b75": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f0c5d43aa414bcb99537c0190cf2d42": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02001e02f7474e36a2d5f1ebf2002b75",
            "max": 6315,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_82eca231657c4e1ca4d5d0a53cdd97a4",
            "value": 33
          }
        },
        "5cdc0fb17be645c88ce20e9271735097": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f1aa841e84144e43a7ad2465c2887e01",
              "IPY_MODEL_4f0c5d43aa414bcb99537c0190cf2d42",
              "IPY_MODEL_dad14fb2851d4925913fbdf9468d73fe"
            ],
            "layout": "IPY_MODEL_8abf3e630b564b29aea4c60760186974"
          }
        },
        "616f3517057d4c038e257d7323d60351": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72f69738faf54f788d43d61f8b98f3f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82eca231657c4e1ca4d5d0a53cdd97a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8abf3e630b564b29aea4c60760186974": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0a939bcd94646ffa4e6d588b283196d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dad14fb2851d4925913fbdf9468d73fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0a939bcd94646ffa4e6d588b283196d",
            "placeholder": "​",
            "style": "IPY_MODEL_72f69738faf54f788d43d61f8b98f3f6",
            "value": " 33/6315 [00:04&lt;28:34,  3.66it/s]"
          }
        },
        "dad27f23524348eb8c2b800f3979178a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f1aa841e84144e43a7ad2465c2887e01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_616f3517057d4c038e257d7323d60351",
            "placeholder": "​",
            "style": "IPY_MODEL_dad27f23524348eb8c2b800f3979178a",
            "value": "Processing XML Files:   1%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
